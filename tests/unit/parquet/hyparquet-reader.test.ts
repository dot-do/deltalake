/**
 * Hyparquet Reader Integration Tests
 *
 * TDD RED Phase: These tests define the expected behavior of parquetReadObjects()
 * integration with deltalake's AsyncBuffer and StorageBackend.
 * All tests should FAIL until the implementation is complete.
 *
 * Tests cover:
 * - parquetReadObjects() integration for reading Parquet files
 * - AsyncBuffer usage for efficient byte-range reads
 * - Partial file reads (specific columns)
 * - Row group filtering via zone maps
 * - Variant column decoding
 * - Integration with StorageBackend.readRange()
 */

import { describe, it, expect, beforeEach } from 'vitest'
import {
  createAsyncBuffer,
  type AsyncBuffer,
  type ZoneMap,
  type ZoneMapFilter,
  canSkipZoneMap,
  decodeVariant,
  encodeVariant,
  type ParquetReadOptions,
} from '../../../src/parquet/index.js'
import {
  MemoryStorage,
  createStorage,
  type StorageBackend,
} from '../../../src/storage/index.js'

// =============================================================================
// MOCK PARQUET FILE DATA
// =============================================================================

/**
 * Creates a minimal valid Parquet file in memory for testing.
 * This is a helper to create test fixtures.
 */
function createMockParquetFile(rows: Record<string, unknown>[]): Uint8Array {
  // PAR1 magic bytes for minimal valid Parquet
  const magic = new Uint8Array([0x50, 0x41, 0x52, 0x31]) // "PAR1"

  // For testing, we'll create a placeholder that simulates a Parquet file
  // In reality, this would need to be generated by hyparquet-writer
  const content = new TextEncoder().encode(JSON.stringify(rows))

  // Minimal Parquet structure: magic + content + footer + magic
  const buffer = new Uint8Array(magic.length + content.length + 8 + magic.length)
  buffer.set(magic, 0)
  buffer.set(content, magic.length)
  // Footer size (4 bytes) - placeholder
  const footerView = new DataView(buffer.buffer)
  footerView.setInt32(buffer.length - 8, content.length, true)
  buffer.set(magic, buffer.length - 4)

  return buffer
}

// =============================================================================
// ASYNC BUFFER CREATION TESTS
// =============================================================================

describe('AsyncBuffer Creation', () => {
  let storage: MemoryStorage

  beforeEach(() => {
    storage = new MemoryStorage()
  })

  describe('createAsyncBuffer from StorageBackend', () => {
    it('should create AsyncBuffer with correct byteLength', async () => {
      const data = new Uint8Array(1024)
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')

      expect(buffer.byteLength).toBe(1024)
    })

    it('should throw error for non-existent file', async () => {
      await expect(createAsyncBuffer(storage, 'missing.parquet')).rejects.toThrow(
        'File not found: missing.parquet'
      )
    })

    it('should return AsyncBuffer that supports slice()', async () => {
      const data = new Uint8Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')

      expect(typeof buffer.slice).toBe('function')
    })

    it('should create AsyncBuffer from any StorageBackend type', async () => {
      const memoryStorage = createStorage({ type: 'memory' })
      await memoryStorage.write('test.parquet', new Uint8Array(100))

      const buffer = await createAsyncBuffer(memoryStorage, 'test.parquet')

      expect(buffer.byteLength).toBe(100)
    })
  })

  describe('AsyncBuffer slice operations', () => {
    it('should read slice from beginning of file', async () => {
      const data = new Uint8Array([0x50, 0x41, 0x52, 0x31, 0x00, 0x01, 0x02, 0x03])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')
      const slice = await buffer.slice(0, 4)

      // PAR1 magic bytes
      expect(new Uint8Array(slice)).toEqual(new Uint8Array([0x50, 0x41, 0x52, 0x31]))
    })

    it('should read slice from middle of file', async () => {
      const data = new Uint8Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')
      const slice = await buffer.slice(3, 7)

      expect(new Uint8Array(slice)).toEqual(new Uint8Array([3, 4, 5, 6]))
    })

    it('should read slice to end of file when end is omitted', async () => {
      const data = new Uint8Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')
      const slice = await buffer.slice(7)

      expect(new Uint8Array(slice)).toEqual(new Uint8Array([7, 8, 9]))
    })

    it('should handle single byte slice', async () => {
      const data = new Uint8Array([0, 1, 2, 3, 4])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')
      const slice = await buffer.slice(2, 3)

      expect(new Uint8Array(slice)).toEqual(new Uint8Array([2]))
    })

    it('should return ArrayBuffer from slice', async () => {
      const data = new Uint8Array([0, 1, 2, 3, 4])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')
      const slice = await buffer.slice(0, 2)

      expect(slice).toBeInstanceOf(ArrayBuffer)
    })
  })
})

// =============================================================================
// PARQUET READ OBJECTS INTEGRATION TESTS
// =============================================================================

describe('parquetReadObjects Integration', () => {
  let storage: MemoryStorage

  beforeEach(() => {
    storage = new MemoryStorage()
  })

  describe('basic read operations', () => {
    it('should read all rows from a Parquet file', async () => {
      // This test requires a real Parquet file written by hyparquet-writer
      // For now, we define the expected behavior
      const expectedRows = [
        { id: 1, name: 'Alice', score: 95.5 },
        { id: 2, name: 'Bob', score: 87.2 },
        { id: 3, name: 'Charlie', score: 91.8 },
      ]

      // TODO: Write actual Parquet file using StreamingParquetWriter
      // const writer = new StreamingParquetWriter()
      // for (const row of expectedRows) await writer.writeRow(row)
      // const result = await writer.finish()
      // await storage.write('test.parquet', new Uint8Array(result.buffer))

      // const buffer = await createAsyncBuffer(storage, 'test.parquet')
      // const rows = await parquetReadObjects({ file: buffer })

      // expect(rows).toHaveLength(3)
      // expect(rows).toEqual(expectedRows)

      // Placeholder assertion until implementation
      expect(expectedRows).toHaveLength(3)
    })

    it('should read rows with correct types', async () => {
      const expectedTypes = {
        id: 'number',
        name: 'string',
        active: 'boolean',
        score: 'number',
      }

      // TODO: Implement with actual Parquet file
      expect(typeof expectedTypes.id).toBe('string')
    })

    it('should handle empty Parquet file', async () => {
      // An empty Parquet file should return an empty array
      const expectedRows: Record<string, unknown>[] = []

      // TODO: Implement with actual empty Parquet file
      expect(expectedRows).toHaveLength(0)
    })

    it('should read Parquet file with single row', async () => {
      const expectedRow = { id: 1, value: 'single' }

      // TODO: Implement with actual Parquet file
      expect(expectedRow.id).toBe(1)
    })
  })

  describe('column selection', () => {
    it('should read only specified columns', async () => {
      // When columns option is specified, only those columns should be read
      const allColumns = ['id', 'name', 'email', 'age', 'score']
      const selectedColumns = ['id', 'name']

      // TODO: Implement with actual Parquet file
      // const buffer = await createAsyncBuffer(storage, 'test.parquet')
      // const rows = await parquetReadObjects({
      //   file: buffer,
      //   columns: selectedColumns,
      // })

      // Each row should only have the selected columns
      // expect(Object.keys(rows[0])).toEqual(selectedColumns)

      expect(selectedColumns).toHaveLength(2)
    })

    it('should read single column', async () => {
      const selectedColumn = 'id'

      // TODO: Implement with actual Parquet file
      // const rows = await parquetReadObjects({
      //   file: buffer,
      //   columns: [selectedColumn],
      // })

      // expect(Object.keys(rows[0])).toEqual([selectedColumn])

      expect(selectedColumn).toBe('id')
    })

    it('should handle non-existent column gracefully', async () => {
      // Reading a column that doesn't exist should throw or return empty
      const invalidColumn = 'nonexistent_column'

      // TODO: Implement with actual Parquet file
      // await expect(parquetReadObjects({
      //   file: buffer,
      //   columns: [invalidColumn],
      // })).rejects.toThrow(/column.*not found/i)

      expect(invalidColumn).toBe('nonexistent_column')
    })

    it('should read nested columns correctly', async () => {
      // Nested columns like 'metadata.userId' should be accessible
      const nestedColumn = 'metadata.userId'

      // TODO: Implement with actual Parquet file containing nested data
      expect(nestedColumn).toContain('.')
    })
  })

  describe('row range selection', () => {
    it('should read rows in specified range (rowStart/rowEnd)', async () => {
      const rowStart = 10
      const rowEnd = 20

      // TODO: Implement with actual Parquet file
      // const rows = await parquetReadObjects({
      //   file: buffer,
      //   rowStart,
      //   rowEnd,
      // })

      // expect(rows).toHaveLength(10)

      expect(rowEnd - rowStart).toBe(10)
    })

    it('should handle rowStart beyond file bounds', async () => {
      // If rowStart is beyond the file, should return empty array
      const rowStart = 1000000

      // TODO: Implement with actual Parquet file
      // const rows = await parquetReadObjects({
      //   file: buffer,
      //   rowStart,
      // })

      // expect(rows).toHaveLength(0)

      expect(rowStart).toBeGreaterThan(0)
    })

    it('should read from rowStart to end when rowEnd is omitted', async () => {
      const rowStart = 50

      // TODO: Implement with actual Parquet file (100 rows)
      // const rows = await parquetReadObjects({
      //   file: buffer,
      //   rowStart,
      // })

      // expect(rows).toHaveLength(50) // 100 - 50

      expect(rowStart).toBe(50)
    })
  })
})

// =============================================================================
// BYTE-RANGE READ EFFICIENCY TESTS
// =============================================================================

describe('Byte-Range Read Efficiency', () => {
  let storage: MemoryStorage

  beforeEach(() => {
    storage = new MemoryStorage()
  })

  describe('efficient footer reading', () => {
    it('should read Parquet footer efficiently (last bytes)', async () => {
      // Parquet stores metadata in the footer (last N bytes)
      // The reader should first read the footer size, then the footer
      const fileSize = 10000
      const data = new Uint8Array(fileSize)
      await storage.write('large.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'large.parquet')

      // Read last 8 bytes (footer size + PAR1 magic)
      const footerEnd = await buffer.slice(fileSize - 8, fileSize)

      expect(new Uint8Array(footerEnd).length).toBe(8)
    })

    it('should avoid reading entire file for metadata', async () => {
      // The createAsyncBuffer should use readRange for byte slices
      // rather than reading the entire file
      const data = new Uint8Array(100000) // 100KB file
      await storage.write('large.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'large.parquet')

      // Simulate footer read - should only read 8 bytes, not entire file
      const footerSlice = await buffer.slice(99992, 100000)

      // The slice should be exactly 8 bytes
      expect(new Uint8Array(footerSlice).length).toBe(8)

      // Verify the byteLength matches the original file
      expect(buffer.byteLength).toBe(100000)
    })
  })

  describe('row group byte range reads', () => {
    it('should read only required row groups', async () => {
      // When reading specific rows, only relevant row groups should be fetched
      // This is a behavioral test - we expect minimal byte ranges to be read

      const trackingStorage = new MemoryStorage()
      const data = new Uint8Array(1000000) // 1MB file with multiple row groups
      await trackingStorage.write('multi-rg.parquet', data)

      // TODO: Implement with actual multi-row-group Parquet file
      // const buffer = await createAsyncBuffer(trackingStorage, 'multi-rg.parquet')
      // await parquetReadObjects({
      //   file: buffer,
      //   rowStart: 5000,
      //   rowEnd: 5100,
      // })

      // Should only read the row group(s) containing rows 5000-5100
      expect(data.length).toBe(1000000)
    })

    it('should coalesce adjacent byte ranges for efficiency', async () => {
      // Adjacent column chunks should be read in a single request
      // when columns are close together in the file

      // TODO: Implement test for byte range coalescing
      expect(true).toBe(true)
    })
  })

  describe('column chunk reading', () => {
    it('should read only required column chunks', async () => {
      // When reading specific columns, only those column chunks should be read

      // TODO: Implement with actual multi-column Parquet file
      // const buffer = await createAsyncBuffer(storage, 'multi-col.parquet')
      // await parquetReadObjects({
      //   file: buffer,
      //   columns: ['id'], // Only read 'id' column
      // })

      expect(true).toBe(true)
    })
  })
})

// =============================================================================
// ROW GROUP FILTERING VIA ZONE MAPS TESTS
// =============================================================================

describe('Row Group Filtering via Zone Maps', () => {
  describe('canSkipZoneMap predicate', () => {
    describe('equality (eq) operator', () => {
      it('should skip row group when value is below min', () => {
        const zoneMap: ZoneMap = { column: 'id', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 50 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should skip row group when value is above max', () => {
        const zoneMap: ZoneMap = { column: 'id', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 250 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when value is within range', () => {
        const zoneMap: ZoneMap = { column: 'id', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 150 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })

      it('should not skip row group when value equals min', () => {
        const zoneMap: ZoneMap = { column: 'id', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 100 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })

      it('should not skip row group when value equals max', () => {
        const zoneMap: ZoneMap = { column: 'id', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 200 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('not-equal (ne) operator', () => {
      it('should skip row group when all values equal the filter value', () => {
        const zoneMap: ZoneMap = { column: 'status', min: 'active', max: 'active', nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'status', operator: 'ne', value: 'active' }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when values differ', () => {
        const zoneMap: ZoneMap = { column: 'status', min: 'active', max: 'pending', nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'status', operator: 'ne', value: 'active' }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('greater-than (gt) operator', () => {
      it('should skip row group when max is less than or equal to value', () => {
        const zoneMap: ZoneMap = { column: 'score', min: 10, max: 50, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'score', operator: 'gt', value: 50 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when max is greater than value', () => {
        const zoneMap: ZoneMap = { column: 'score', min: 10, max: 50, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'score', operator: 'gt', value: 30 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('greater-than-or-equal (gte) operator', () => {
      it('should skip row group when max is less than value', () => {
        const zoneMap: ZoneMap = { column: 'score', min: 10, max: 50, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'score', operator: 'gte', value: 51 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when max equals value', () => {
        const zoneMap: ZoneMap = { column: 'score', min: 10, max: 50, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'score', operator: 'gte', value: 50 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('less-than (lt) operator', () => {
      it('should skip row group when min is greater than or equal to value', () => {
        const zoneMap: ZoneMap = { column: 'age', min: 30, max: 60, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'age', operator: 'lt', value: 30 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when min is less than value', () => {
        const zoneMap: ZoneMap = { column: 'age', min: 30, max: 60, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'age', operator: 'lt', value: 50 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('less-than-or-equal (lte) operator', () => {
      it('should skip row group when min is greater than value', () => {
        const zoneMap: ZoneMap = { column: 'age', min: 30, max: 60, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'age', operator: 'lte', value: 29 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when min equals value', () => {
        const zoneMap: ZoneMap = { column: 'age', min: 30, max: 60, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'age', operator: 'lte', value: 30 }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('in operator', () => {
      it('should skip row group when all values are outside range', () => {
        const zoneMap: ZoneMap = { column: 'category', min: 10, max: 20, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'category', operator: 'in', value: [1, 2, 3, 25, 30] }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when any value is in range', () => {
        const zoneMap: ZoneMap = { column: 'category', min: 10, max: 20, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'category', operator: 'in', value: [1, 2, 15, 25] }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })

      it('should handle empty in array', () => {
        const zoneMap: ZoneMap = { column: 'category', min: 10, max: 20, nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'category', operator: 'in', value: [] }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })
    })

    describe('between operator', () => {
      it('should skip row group when range is completely below zone map', () => {
        const zoneMap: ZoneMap = { column: 'price', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = {
          column: 'price',
          operator: 'between',
          value: 10,
          value2: 50,
        }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should skip row group when range is completely above zone map', () => {
        const zoneMap: ZoneMap = { column: 'price', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = {
          column: 'price',
          operator: 'between',
          value: 250,
          value2: 500,
        }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })

      it('should not skip row group when ranges overlap', () => {
        const zoneMap: ZoneMap = { column: 'price', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = {
          column: 'price',
          operator: 'between',
          value: 150,
          value2: 250,
        }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })

      it('should not skip row group when filter range contains zone map', () => {
        const zoneMap: ZoneMap = { column: 'price', min: 100, max: 200, nullCount: 0 }
        const filter: ZoneMapFilter = {
          column: 'price',
          operator: 'between',
          value: 50,
          value2: 250,
        }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('string comparisons', () => {
      it('should handle string min/max correctly', () => {
        const zoneMap: ZoneMap = { column: 'name', min: 'Alice', max: 'Charlie', nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'name', operator: 'eq', value: 'Bob' }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })

      it('should skip when string is lexicographically outside range', () => {
        const zoneMap: ZoneMap = { column: 'name', min: 'Bob', max: 'David', nullCount: 0 }
        const filter: ZoneMapFilter = { column: 'name', operator: 'eq', value: 'Alice' }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })
    })

    describe('date comparisons', () => {
      it('should handle Date min/max correctly', () => {
        const zoneMap: ZoneMap = {
          column: 'created_at',
          min: new Date('2024-01-01'),
          max: new Date('2024-12-31'),
          nullCount: 0,
        }
        const filter: ZoneMapFilter = {
          column: 'created_at',
          operator: 'eq',
          value: new Date('2024-06-15'),
        }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })

      it('should skip when date is outside range', () => {
        const zoneMap: ZoneMap = {
          column: 'created_at',
          min: new Date('2024-01-01'),
          max: new Date('2024-12-31'),
          nullCount: 0,
        }
        const filter: ZoneMapFilter = {
          column: 'created_at',
          operator: 'eq',
          value: new Date('2023-06-15'),
        }

        expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
      })
    })

    describe('non-comparable types', () => {
      it('should not skip for non-comparable min/max', () => {
        const zoneMap: ZoneMap = {
          column: 'data',
          min: { complex: 'object' },
          max: { complex: 'object' },
          nullCount: 0,
        }
        const filter: ZoneMapFilter = { column: 'data', operator: 'eq', value: 'test' }

        // Should return false (don't skip) because we can't compare objects
        expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
      })
    })

    describe('null/undefined handling', () => {
      describe('zoneMap.min null/undefined', () => {
        it('should not skip when zoneMap.min is null', () => {
          const zoneMap: ZoneMap = { column: 'id', min: null as unknown as number, max: 200, nullCount: 5 }
          const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 150 }

          // Cannot determine bounds with null min, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip when zoneMap.min is undefined', () => {
          const zoneMap: ZoneMap = { column: 'id', min: undefined as unknown as number, max: 200, nullCount: 5 }
          const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 150 }

          // Cannot determine bounds with undefined min, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })
      })

      describe('zoneMap.max null/undefined', () => {
        it('should not skip when zoneMap.max is null', () => {
          const zoneMap: ZoneMap = { column: 'id', min: 100, max: null as unknown as number, nullCount: 5 }
          const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 150 }

          // Cannot determine bounds with null max, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip when zoneMap.max is undefined', () => {
          const zoneMap: ZoneMap = { column: 'id', min: 100, max: undefined as unknown as number, nullCount: 5 }
          const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 150 }

          // Cannot determine bounds with undefined max, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })
      })

      describe('filter.value null/undefined', () => {
        it('should not skip when filter.value is null', () => {
          const zoneMap: ZoneMap = { column: 'id', min: 100, max: 200, nullCount: 0 }
          const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: null }

          // Cannot compare with null filter value, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip when filter.value is undefined', () => {
          const zoneMap: ZoneMap = { column: 'id', min: 100, max: 200, nullCount: 0 }
          const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: undefined }

          // Cannot compare with undefined filter value, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip for gt operator with null filter value', () => {
          const zoneMap: ZoneMap = { column: 'score', min: 10, max: 50, nullCount: 0 }
          const filter: ZoneMapFilter = { column: 'score', operator: 'gt', value: null }

          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip for lt operator with undefined filter value', () => {
          const zoneMap: ZoneMap = { column: 'score', min: 10, max: 50, nullCount: 0 }
          const filter: ZoneMapFilter = { column: 'score', operator: 'lt', value: undefined }

          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })
      })

      describe('between operator with null/undefined value2', () => {
        it('should not skip when filter.value2 is null', () => {
          const zoneMap: ZoneMap = { column: 'price', min: 100, max: 200, nullCount: 0 }
          const filter: ZoneMapFilter = {
            column: 'price',
            operator: 'between',
            value: 50,
            value2: null,
          }

          // Cannot determine range with null value2, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip when filter.value2 is undefined', () => {
          const zoneMap: ZoneMap = { column: 'price', min: 100, max: 200, nullCount: 0 }
          const filter: ZoneMapFilter = {
            column: 'price',
            operator: 'between',
            value: 50,
            value2: undefined,
          }

          // Cannot determine range with undefined value2, conservatively do not skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip when filter.value is null for between', () => {
          const zoneMap: ZoneMap = { column: 'price', min: 100, max: 200, nullCount: 0 }
          const filter: ZoneMapFilter = {
            column: 'price',
            operator: 'between',
            value: null,
            value2: 150,
          }

          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })
      })

      describe('in operator with null/undefined values in array', () => {
        it('should not skip when array contains null (conservative approach)', () => {
          const zoneMap: ZoneMap = { column: 'category', min: 10, max: 20, nullCount: 0 }
          // Array with null - should not skip because null could potentially match
          const filter: ZoneMapFilter = { column: 'category', operator: 'in', value: [1, 2, null, 25] }

          // Should NOT skip because null is treated conservatively
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip when array contains undefined (conservative approach)', () => {
          const zoneMap: ZoneMap = { column: 'category', min: 10, max: 20, nullCount: 0 }
          // Array with undefined - should not skip because undefined could potentially match
          const filter: ZoneMapFilter = { column: 'category', operator: 'in', value: [1, 2, undefined, 25] }

          // Should NOT skip because undefined is treated conservatively
          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should skip when all comparable values are outside range (no nulls)', () => {
          const zoneMap: ZoneMap = { column: 'category', min: 10, max: 20, nullCount: 0 }
          const filter: ZoneMapFilter = { column: 'category', operator: 'in', value: [1, 2, 3, 25, 30] }

          // All values outside range, should skip
          expect(canSkipZoneMap(zoneMap, filter)).toBe(true)
        })
      })

      describe('both min and max null/undefined', () => {
        it('should not skip when both min and max are null', () => {
          const zoneMap: ZoneMap = {
            column: 'id',
            min: null as unknown as number,
            max: null as unknown as number,
            nullCount: 100,
          }
          const filter: ZoneMapFilter = { column: 'id', operator: 'eq', value: 150 }

          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })

        it('should not skip for ne operator when min and max are null', () => {
          const zoneMap: ZoneMap = {
            column: 'status',
            min: null as unknown as string,
            max: null as unknown as string,
            nullCount: 50,
          }
          const filter: ZoneMapFilter = { column: 'status', operator: 'ne', value: 'active' }

          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })
      })

      describe('all three values null/undefined', () => {
        it('should not skip when min, max, and filter.value are all null', () => {
          const zoneMap: ZoneMap = {
            column: 'data',
            min: null as unknown as number,
            max: null as unknown as number,
            nullCount: 100,
          }
          const filter: ZoneMapFilter = { column: 'data', operator: 'eq', value: null }

          expect(canSkipZoneMap(zoneMap, filter)).toBe(false)
        })
      })
    })
  })

  describe('zone map integration with reading', () => {
    it('should skip row groups based on zone map predicates', async () => {
      // TODO: Integration test with actual Parquet file
      // const buffer = await createAsyncBuffer(storage, 'indexed.parquet')
      // const rows = await parquetReadObjects({
      //   file: buffer,
      //   filter: { column: 'id', operator: 'eq', value: 500 },
      // })

      // Only row groups containing id=500 should be read
      expect(true).toBe(true)
    })

    it('should read all row groups when no filter is provided', async () => {
      // Without a filter, all row groups should be read
      expect(true).toBe(true)
    })
  })
})

// =============================================================================
// VARIANT COLUMN DECODING TESTS
// =============================================================================

describe('Variant Column Decoding', () => {
  describe('encodeVariant and decodeVariant roundtrip', () => {
    it('should roundtrip null values', () => {
      const original = null
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBeNull()
    })

    it('should roundtrip boolean true', () => {
      const original = true
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(true)
    })

    it('should roundtrip boolean false', () => {
      const original = false
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(false)
    })

    it('should roundtrip small integers (int8)', () => {
      const original = 42
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(42)
    })

    it('should roundtrip negative integers', () => {
      const original = -100
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(-100)
    })

    it('should roundtrip large integers (int32)', () => {
      const original = 1000000
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(1000000)
    })

    it('should roundtrip bigint (int64)', () => {
      const original = BigInt('9223372036854775807')
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(original)
    })

    it('should roundtrip floating point numbers', () => {
      const original = 3.14159265359
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBeCloseTo(original, 10)
    })

    it('should roundtrip short strings', () => {
      const original = 'hello'
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe('hello')
    })

    it('should roundtrip long strings', () => {
      const original = 'a'.repeat(100)
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(original)
    })

    it('should roundtrip unicode strings', () => {
      const original = '\u4e2d\u6587\u65e5\u672c\u8a9e\ud55c\uad6d\uc5b4'
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toBe(original)
    })

    it('should roundtrip Date values', () => {
      const original = new Date('2024-06-15T10:30:00Z')
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual(original)
    })

    it('should roundtrip binary data (Uint8Array)', () => {
      const original = new Uint8Array([0x00, 0x01, 0x02, 0xff, 0xfe])
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual(original)
    })

    it('should roundtrip empty arrays', () => {
      const original: unknown[] = []
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual([])
    })

    it('should roundtrip arrays of primitives', () => {
      const original = [1, 2, 3, 'four', true, null]
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual(original)
    })

    it('should roundtrip nested arrays', () => {
      const original = [[1, 2], [3, [4, 5]], []]
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual(original)
    })

    it('should roundtrip empty objects', () => {
      const original = {}
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual({})
    })

    it('should roundtrip simple objects', () => {
      const original = { name: 'Alice', age: 30, active: true }
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual(original)
    })

    it('should roundtrip nested objects', () => {
      const original = {
        user: {
          name: 'Bob',
          address: {
            city: 'NYC',
            zip: '10001',
          },
        },
      }
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual(original)
    })

    it('should roundtrip complex mixed structures', () => {
      const original = {
        string: 'hello',
        number: 42,
        float: 3.14,
        boolean: true,
        null: null,
        array: [1, 'two', { three: 3 }],
        nested: { deep: { deeper: { value: 'found' } } },
      }
      const encoded = encodeVariant(original)
      const decoded = decodeVariant(encoded)

      expect(decoded).toEqual(original)
    })
  })

  describe('variant column reading from Parquet', () => {
    it('should decode variant column containing JSON objects', async () => {
      // TODO: Integration test with actual Parquet file containing variant column
      // const writer = new StreamingParquetWriter()
      // await writer.writeRow({
      //   id: 1,
      //   metadata: { key: 'value', nested: { deep: true } },
      // })
      // const result = await writer.finish()
      // await storage.write('variant.parquet', new Uint8Array(result.buffer))

      // const buffer = await createAsyncBuffer(storage, 'variant.parquet')
      // const rows = await parquetReadObjects({ file: buffer })

      // expect(rows[0].metadata).toEqual({ key: 'value', nested: { deep: true } })

      expect(true).toBe(true)
    })

    it('should decode variant column containing arrays', async () => {
      // TODO: Integration test
      expect(true).toBe(true)
    })

    it('should handle variant columns with null values', async () => {
      // TODO: Integration test
      expect(true).toBe(true)
    })

    it('should decode variant with binary data', async () => {
      // TODO: Integration test
      expect(true).toBe(true)
    })
  })

  describe('variant shredding for statistics', () => {
    it('should use shredded variant field statistics for filtering', async () => {
      // When variant fields are shredded, their statistics should be used
      // for predicate pushdown

      // TODO: Integration test with shredded variant column
      // const writer = new StreamingParquetWriter({
      //   shredFields: ['data.category', 'data.price'],
      // })
      // for (let i = 0; i < 1000; i++) {
      //   await writer.writeRow({
      //     data: { category: i % 10, price: i * 10 },
      //   })
      // }

      // Filter on shredded field should use zone maps
      // const rows = await parquetReadObjects({
      //   file: buffer,
      //   filter: { 'data.category': { $eq: 5 } },
      // })

      expect(true).toBe(true)
    })
  })
})

// =============================================================================
// STORAGE BACKEND INTEGRATION TESTS
// =============================================================================

describe('StorageBackend Integration', () => {
  describe('MemoryStorage with parquetReadObjects', () => {
    let storage: MemoryStorage

    beforeEach(() => {
      storage = new MemoryStorage()
    })

    it('should read Parquet file from MemoryStorage', async () => {
      // TODO: Write and read Parquet file using MemoryStorage
      expect(storage).toBeDefined()
    })

    it('should use slice operations for byte-range reads', async () => {
      const data = new Uint8Array(10000)
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')

      // Simulate footer read - should use readRange internally
      const slice = await buffer.slice(9992, 10000)

      // Verify slice returns the correct bytes
      expect(new Uint8Array(slice).length).toBe(8)

      // The AsyncBuffer should support efficient byte-range access
      expect(buffer.byteLength).toBe(10000)
    })
  })

  describe('createAsyncBuffer with different storage backends', () => {
    it('should work with MemoryStorage', async () => {
      const storage = createStorage({ type: 'memory' })
      await storage.write('test.parquet', new Uint8Array(100))

      const buffer = await createAsyncBuffer(storage, 'test.parquet')

      expect(buffer.byteLength).toBe(100)
    })

    it('should maintain correct byteLength after file update', async () => {
      const storage = new MemoryStorage()

      await storage.write('test.parquet', new Uint8Array(100))
      const buffer1 = await createAsyncBuffer(storage, 'test.parquet')
      expect(buffer1.byteLength).toBe(100)

      await storage.write('test.parquet', new Uint8Array(200))
      const buffer2 = await createAsyncBuffer(storage, 'test.parquet')
      expect(buffer2.byteLength).toBe(200)
    })
  })
})

// =============================================================================
// ERROR HANDLING TESTS
// =============================================================================

describe('Error Handling', () => {
  let storage: MemoryStorage

  beforeEach(() => {
    storage = new MemoryStorage()
  })

  describe('invalid Parquet files', () => {
    it('should throw error for file without PAR1 magic', async () => {
      const invalidData = new Uint8Array([0x00, 0x00, 0x00, 0x00])
      await storage.write('invalid.parquet', invalidData)

      // TODO: Implement with actual parquetReadObjects
      // const buffer = await createAsyncBuffer(storage, 'invalid.parquet')
      // await expect(parquetReadObjects({ file: buffer })).rejects.toThrow(/magic/i)

      expect(invalidData[0]).toBe(0)
    })

    it('should throw error for corrupted footer', async () => {
      // File with valid magic but invalid footer
      const corruptedData = new Uint8Array([
        0x50, 0x41, 0x52, 0x31, // PAR1
        0xff, 0xff, 0xff, 0xff, // Invalid data
        0x50, 0x41, 0x52, 0x31, // PAR1
      ])
      await storage.write('corrupted.parquet', corruptedData)

      // TODO: Implement test
      expect(corruptedData.length).toBe(12)
    })

    it('should throw error for truncated file', async () => {
      // File that ends abruptly
      const truncatedData = new Uint8Array([0x50, 0x41, 0x52, 0x31])
      await storage.write('truncated.parquet', truncatedData)

      // TODO: Implement test
      expect(truncatedData.length).toBe(4)
    })
  })

  describe('storage errors during read', () => {
    it('should propagate storage read errors', async () => {
      // Reading from non-existent file should throw
      await expect(createAsyncBuffer(storage, 'missing.parquet')).rejects.toThrow(
        'File not found: missing.parquet'
      )
    })
  })

  describe('slice boundary errors', () => {
    it('should handle slice beyond file boundary', async () => {
      const data = new Uint8Array([1, 2, 3, 4, 5])
      await storage.write('small.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'small.parquet')

      // Requesting bytes beyond file size
      const slice = await buffer.slice(0, 10)

      // Should return available bytes without error
      expect(new Uint8Array(slice).length).toBeLessThanOrEqual(10)
    })

    it('should handle negative slice start', async () => {
      const data = new Uint8Array([1, 2, 3, 4, 5])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')

      // Negative start should be treated as 0 or throw
      // Behavior depends on implementation
      expect(buffer.byteLength).toBe(5)
    })
  })
})

// =============================================================================
// PERFORMANCE CHARACTERISTICS TESTS
// =============================================================================

describe('Performance Characteristics', () => {
  describe('memory efficiency', () => {
    it('should not load entire file into memory for metadata read', async () => {
      const storage = new MemoryStorage()
      const largeFile = new Uint8Array(10 * 1024 * 1024) // 10MB
      await storage.write('large.parquet', largeFile)

      // TODO: Test that metadata read uses byte ranges
      expect(largeFile.length).toBe(10 * 1024 * 1024)
    })

    it('should stream row groups instead of loading all at once', async () => {
      // TODO: Test streaming behavior
      expect(true).toBe(true)
    })
  })

  describe('concurrent reads', () => {
    it('should handle concurrent reads from same file', async () => {
      const storage = new MemoryStorage()
      const data = new Uint8Array(1000)
      await storage.write('concurrent.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'concurrent.parquet')

      // Multiple concurrent slice operations
      const reads = await Promise.all([
        buffer.slice(0, 100),
        buffer.slice(100, 200),
        buffer.slice(200, 300),
        buffer.slice(300, 400),
      ])

      expect(reads).toHaveLength(4)
      reads.forEach(slice => {
        expect(new Uint8Array(slice).length).toBe(100)
      })
    })
  })
})

// =============================================================================
// HYPARQUET SPECIFIC INTEGRATION TESTS
// =============================================================================

describe('Hyparquet Specific Integration', () => {
  describe('AsyncBuffer interface compatibility', () => {
    it('should be compatible with hyparquet AsyncBuffer interface', async () => {
      const storage = new MemoryStorage()
      await storage.write('test.parquet', new Uint8Array(100))

      const buffer = await createAsyncBuffer(storage, 'test.parquet')

      // Verify interface compliance
      expect(typeof buffer.byteLength).toBe('number')
      expect(typeof buffer.slice).toBe('function')

      // slice should return something that can be awaited
      const sliceResult = buffer.slice(0, 10)
      expect(sliceResult).toBeDefined()
    })

    it('should return ArrayBuffer or Uint8Array from slice', async () => {
      const storage = new MemoryStorage()
      const data = new Uint8Array([1, 2, 3, 4, 5])
      await storage.write('test.parquet', data)

      const buffer = await createAsyncBuffer(storage, 'test.parquet')
      const slice = await buffer.slice(0, 3)

      // Should be ArrayBuffer (as per hyparquet interface)
      expect(slice).toBeInstanceOf(ArrayBuffer)
    })
  })

  describe('parquetReadOptions compatibility', () => {
    it('should support rowFormat: object option', async () => {
      // parquetReadObjects should return objects (not arrays)
      // TODO: Test with actual Parquet file
      expect(true).toBe(true)
    })

    it('should support columns option for column projection', async () => {
      // TODO: Test column projection
      expect(true).toBe(true)
    })

    it('should support rowStart/rowEnd for row range', async () => {
      // TODO: Test row range selection
      expect(true).toBe(true)
    })
  })

  describe('compression codec support', () => {
    it('should handle SNAPPY compressed Parquet files', async () => {
      // TODO: Test with SNAPPY compressed file
      expect(true).toBe(true)
    })

    it('should handle ZSTD compressed Parquet files', async () => {
      // TODO: Test with ZSTD compressed file
      expect(true).toBe(true)
    })

    it('should handle UNCOMPRESSED Parquet files', async () => {
      // TODO: Test with uncompressed file
      expect(true).toBe(true)
    })
  })
})
